```markdown
Propuesta integral (adaptada a un artículo de innovación curricular en Journal of Statistics and Data Science Education): enseñar inferencia causal desde experimentos controlados hasta “IA causal” (SCM/DAG y búsqueda causal)Nota de alcance y evidencia disponibleLa propuesta original incluye “causal ML/AI”. Con las referencias disponibles, puedo sustentar con evidencia (i) la unificación de enfoques estructurales, contrafactuales y manipulativos, incluyendo herramientas gráficas y el marco de Pearl (donde se incluye el do-cálculo) Pearl, 1995; , (ii) el uso de DAG/SCM como “lenguaje matemático” para integrar supuestos y conectar con identificación (Pearl, 2009; , Pearl, 1995; , (iii) la conexión entre independencia condicional y estructura causal y los procedimientos de búsqueda/descubrimiento causal típicos del cruce estadística–IA West, 2009), y (iv) componentes aplicados clave (diseños alternativos y fallas prácticas en RCT, discrepancias RCT vs no-RCT, datos faltantes/doble robustez, preferencias/DRPT, distribuciones contrafactuales, y retos en series temporales) Ioannidis (2001)–Asesh (2022). No puedo respaldar con estas fuentes afirmaciones específicas sobre técnicas modernas de “causal ML” (p. ej., double machine learning) porque no están en el set provisto; por ello, el componente “IA causal” se operacionaliza aquí como SCM/DAG + do-cálculo + búsqueda causal basada en independencia condicional (la parte más sólidamente sustentada por Pearl, 1995; , (Pearl, 2009; , West, 2009)).1. Enfoque del manuscrito: “innovación didáctica” con artefactos reproduciblesTipo de artículo propuesto: innovación curricular / guía pedagógica con (a) secuencia instruccional, (b) laboratorios reproducibles, y (c) evaluación de aprendizaje y de calidad de especificación causal.Contribución central (reformulación publicable de tu idea): en vez de un “lenguaje unificado” como tratado abstracto, entregar un pipeline de especificación causal que traduzca sistemáticamente:
1) pregunta en lenguaje natural → 2) intervención/estimando contrafactual → 3) DAG/SCM con supuestos → 4) estrategia de identificación (vía criterios gráficos/do-cálculo en el marco de Pearl) → 5) implementación estadística y diagnóstico de amenazas prácticas (no cumplimiento/atrición, evidencia observacional vs experimental, missingness, preferencias, series temporales) (Pearl, 2009; , Pearl, 1995; , Ioannidis (2001)–Asesh (2022).Justificación: Pearl plantea explícitamente que los diagramas causales son un lenguaje matemático para integrar información estadística y sustantiva y para “consultar” si los supuestos bastan para identificar efectos causales en datos no experimentales, y, si es posible, derivar expresiones del efecto en términos observables (y si no, sugerir observaciones o experimentos auxiliares) (Pearl, 2009; . Además, su tratado posterior articula una teoría que unifica enfoques probabilísticos, manipulativos, contrafactuales y estructurales, lo que fundamenta pedagógicamente una enseñanza “traductora” entre paradigmas sin perder rigor Pearl, 1995; . Spirtes et al. muestran cómo la conexión independencia condicional–estructura causal es esencial para procedimientos de búsqueda causal, ofreciendo un puente natural hacia la dimensión “IA” (causal discovery) dentro de un curso de estadística/ciencia de datos West, 2009).2. Objetivos de aprendizaje (Learning Outcomes) alineados con la literatura causalAl finalizar la secuencia, el estudiantado será capaz de:LO1 (Especificación): Formular una pregunta causal como contraste entre escenarios/intervenciones, distinguiéndola de asociación estadística, y declarar el estimando (qué efecto causal se busca) en términos contrafactuales Pearl, 1995; , (Pearl, 2009; , West, 2009).LO2 (Lenguaje gráfico): Construir un DAG/SCM mínimo que codifique supuestos causales (confusión, mediación, colisión) y usarlo como lenguaje para justificar decisiones analíticas (p. ej., qué ajustar y qué no) (Pearl, 2009; , Pearl, 1995; .LO3 (Identificación como “diagnóstico” didáctico): Determinar si el efecto es identificable bajo los supuestos declarados (y, si aplica, derivar la expresión de ajuste mediante razonamiento gráfico/do-cálculo en el marco de Pearl), o justificar por qué no lo es y qué datos/experimentos auxiliares se requerirían (Pearl, 2009; , Pearl, 1995; .LO4 (Diseño y validez): Explicar por qué los RCT son preferibles cuando factibles, pero también por qué fallan supuestos (atrición, no cumplimiento) y por qué se recurre a diseños alternativos; discutir la incertidumbre adicional de estimaciones en alternativas Ioannidis (2001).LO5 (Evidencia experimental vs observacional): Interpretar y discutir discrepancias empíricas entre evidencia aleatorizada y no aleatorizada, vinculándolas con supuestos/diseños y con la necesidad de especificación transparente Marcus et al. (2012), Ioannidis (2001).LO6 (Amenazas comunes y extensiones):Missingness: justificar por qué la falta de datos amenaza validez y aplicar un enfoque doblemente robusto que combina modelo de resultado y mecanismo de falta, vinculándolo con supuestos explícitos Bang & Robins (2005), Ioannidis (2001).Preferencia/adhesión: reconocer cuándo el estimando relevante es “randomización vs preferencia” y cómo diseños híbridos (DRPT) permiten estimarlo, clarificando la generalización Spirtes et al. (2001).Más allá de la media: formular efectos sobre la distribución contrafactual (cuantiles/distribución completa) y conectarlos con métodos de regresión e inferencia funcional Chernozhukov et al. (2009).Series temporales: describir retos particulares (ruido, eficiencia, alta dimensión) y cómo la especificación causal (dirección, retardos, cadenas) es parte del problema Asesh (2022).3. Diseño instruccional propuesto (módulos, actividades, productos)La secuencia está pensada para 6–8 semanas (adaptable). Cada módulo tiene (i) concepto, (ii) práctica guiada, (iii) producto evaluable con rúbrica.Módulo 1 — De asociación a causalidad: qué significa “efecto” en un curso de estadísticaConcepto: unificación de perspectivas (probabilística/estructural/contrafactual/manipulativa) como base para definir “pregunta causal” de manera no ambigua Pearl, 1995; .Actividad: reescribir preguntas típicas de regresión (“X predice Y”) a forma intervencional (“¿qué pasaría con Y si intervenimos X?”), conectando con la idea de traducción entre paradigmas Pearl, 1995; , West, 2009).Producto: “Ficha de especificación causal v0” (pregunta, población, intervención, outcome, estimando tentativo).Módulo 2 — DAG/SCM como lenguaje matemático de supuestosConcepto: DAG como lenguaje para integrar conocimiento sustantivo y estadístico; consulta del grafo para evaluar suficiencia de supuestos e informar identificación/observación adicional (Pearl, 2009; .Actividad: construir DAG para 3 viñetas aplicadas (salud/educación/política), identificando confusores, mediadores y colisionadores.Producto: DAG mínimo + lista explícita de supuestos (p. ej., ausencia de confusión no medida) anclada al diagrama (Pearl, 2009; , Pearl, 1995; .Módulo 3 — Identificación como diagnóstico: de DAG a expresión (y al “no se puede”)Concepto: uso del marco de Pearl para pasar del grafo a expresiones de efecto causal cuando es identificable; si no lo es, qué observaciones/experimentos auxiliares ayudarían (Pearl, 2009; , Pearl, 1995; .Actividad: ejercicios de “backdoor/frontdoor” (cuando aplique) y discusión de cuándo una regresión ajustada es consistente con el DAG.Producto: “Justificación de ajuste” (por qué ajustar un set y por qué NO ajustar otro), referenciando el DAG como argumento formal (Pearl, 2009; .Módulo 4 — Diseño: RCT, fallas prácticas y alternativasConcepto: por qué el RCT es preferible si factible, pero supuestos pueden fallar (atrición, no cumplimiento) y pueden ser inviables/antiéticos; alternativas permiten responder más preguntas pero con mayor incertidumbre en magnitudes Ioannidis (2001).Actividad: rediseñar un RCT imposible a un diseño alternativo; listar amenazas a la validez y cómo la especificación causal cambia.Producto: memo de diseño (RCT vs alternativa) conectando el estimando con el diseño Ioannidis (2001), Pearl, 1995; .Módulo 5 — Por qué importa la especificación: discrepancias RCT vs observacionalConcepto: evidencia de que, aunque haya correlación entre RCT y estudios no aleatorizados, son frecuentes discrepancias en magnitudes de efecto más allá del azar Marcus et al. (2012).Actividad: lectura guiada y debate: “¿qué supuestos/diseños podrían explicar discrepancias?” conectándolo a la necesidad de hacer explícito el modelo causal Ioannidis (2001), (Pearl, 2009; .Producto: crítica estructurada de un estudio observacional (qué supuestos faltan, qué DAG implícito hay).Módulo 6 — Amenazas y extensiones (tres laboratorios “ciencia de datos causal”)Lab 6A: Missingness y doble robustezBase: missing outcome como amenaza principal y estimación doblemente robusta usando (i) regresión del outcome y (ii) modelo del mecanismo de falta; consistencia si al menos uno está bien especificado Bang & Robins (2005).Producto: notebook reproducible con comparación “análisis ingenuo vs doble robustez” y declaración explícita de supuestos Bang & Robins (2005), Ioannidis (2001).Lab 6B: Preferencia/adhesión y generalización (DRPT)Base: DRPT estima el efecto causal de asignación aleatoria vs elección/preferencia; motivación: compromiso/motivación puede cambiar con preferencia y afectar generalización Spirtes et al. (2001), en línea con preocupaciones prácticas del diseño Ioannidis (2001).Producto: simulación o reanálisis simple con interpretación del estimando “randomization vs preference” Spirtes et al. (2001).Lab 6C: Distribuciones contrafactuales (más allá de ATE)Base: herramientas de inferencia para distribuciones contrafactuales usando métodos de regresión (incluyendo distribución completa y cuantiles) y construcción de bandas de confianza funcionales Chernozhukov et al. (2009).Producto: reporte breve que compare “efecto promedio” vs “efectos en cuantiles/distribución”, explicitando el escenario contrafactual Chernozhukov et al. (2009), Pearl, 1995; .(Opcional) Módulo 7 — Series temporales y causalidad: especificación bajo ruido y alta dimensiónBase: retos de inferir causalidad desde series temporales observacionales: resiliencia al ruido, eficiencia computacional e inferencia en alta dimensión; además de dirección causal, retardos y cadenas Asesh (2022).Producto: mini-proyecto con especificación de lags/cadena causal y discusión de supuestos Asesh (2022), (Pearl, 2009; .(Transversal) Componente “IA causal” sustentado: búsqueda causal / independencia condicionalBase: Spirtes et al. enfatizan que conectar independencia condicional con estructura causal es esencial para procedimientos de descubrimiento causal y discuten conexiones con el marco de Rubin, constituyendo el puente natural a la parte “AI” en un curso de datos West, 2009), coherente con el rol central de grafos/estructura en el enfoque de Pearl (Pearl, 2009; , Pearl, 1995; .Actividad transversal: ejercicios donde se comparan (i) DAG propuesto por conocimiento de dominio vs (ii) hipótesis de estructura sugerida por patrones de independencia condicional (a nivel conceptual, sin prometer software específico) West, 2009), (Pearl, 2009; .4. Diseño de evaluación del aprendizaje (para reportarlo como paper)Para que el artículo sea publicable como innovación educativa, la evaluación debe mostrar evidencia de logro de outcomes con instrumentos transparentes. Aquí se propone una evaluación “ligera” y compatible con limitaciones reales (atrition/no compliance), que West discute como problemas frecuentes incluso en contextos experimentales Ioannidis (2001), y que puede beneficiarse de enfoques que hagan explícitas amenazas (p. ej., missingness) Bang & Robins (2005).4.1. Datos a recolectar (mínimo viable)Pre-test / post-test conceptual sobre: especificación causal, lectura de DAG, identificación como diagnóstico, y amenazas (missingness, discrepancias diseño-evidencia) (Pearl, 2009; , Marcus et al. (2012), Bang & Robins (2005).Artefactos evaluables: fichas de especificación, DAGs, justificaciones de ajuste, notebooks de labs (DR, DRPT, contrafactual distributions) (Pearl, 2009; , Bang & Robins (2005)–Chernozhukov et al. (2009).Eventos de no cumplimiento/atrición del curso (si los hay) como parte del reporte de validez interna/externa de la evaluación, consistente con preocupaciones prácticas descritas por West Ioannidis (2001).4.2. Análisis sugerido (simple, defendible)Comparación pre vs post en puntajes de rúbrica (medias y distribuciones), con discusión de amenazas si hay atrición/no respuesta (en línea con los problemas de atrición y falta de datos subrayados por West y Bang & Robins) Ioannidis (2001), Bang & Robins (2005).Análisis descriptivo de errores típicos: ajustes indebidos (p. ej., ajustar colisionadores), preguntas no intervencionales, ausencia de supuestos, etc., usando el DAG como referencia formal (Pearl, 2009; , Pearl, 1995; .Discusión de por qué la mejora en especificación es relevante para interpretar discrepancias entre evidencias experimentales y no experimentales (Ioannidis) Marcus et al. (2012) y para seleccionar diseños alternativos con claridad sobre incertidumbre (West) Ioannidis (2001).5. Pipeline paso a paso para ejecutar el proyecto (y convertirlo en artículo)A continuación, un pipeline operativo (12 pasos) para que el proyecto sea ejecutable en un semestre y quede listo para enviar.Fase A — Preparación (4–6 semanas)Paso 1. Alineación editorial (1 semana).Definir el formato como “innovación curricular” con materiales reproducibles (plantillas, rúbricas, notebooks). Enfatizar que el DAG es el lenguaje integrador y consultable para supuestos/identificación (Pearl, 2009; , dentro del marco unificador de causalidad Pearl, 1995; , y con puente a búsqueda causal (IA) vía independencia condicional West, 2009).Paso 2. Definir outcomes y mapa de contenidos (1 semana).Mapear LO1–LO6 (y LO7 opcional) a módulos/actividades, asegurando cobertura explícita de: diseños alternativos/limitaciones del RCT Ioannidis (2001), discrepancias RCT vs no-RCT Marcus et al. (2012), missingness/doble robustez Bang & Robins (2005), preferencia/DRPT Spirtes et al. (2001), distribuciones contrafactuales Chernozhukov et al. (2009), y series temporales Asesh (2022).Paso 3. Construir la “Ficha de especificación causal” (1 semana).Plantilla de 1–2 páginas: (i) pregunta en lenguaje natural, (ii) intervención, (iii) población, (iv) outcome, (v) estimando, (vi) DAG, (vii) estrategia de identificación/diagnóstico, (viii) amenazas y mitigaciones (missingness, no cumplimiento, preferencia) (Pearl, 2009; , Pearl, 1995; , Ioannidis (2001), Bang & Robins (2005), Spirtes et al. (2001).Paso 4. Diseñar instrumentos de evaluación (1–2 semanas).Pre/post test con ítems anclados a los principios de DAG como lenguaje y a la unificación conceptual (Pearl, 2009; , Pearl, 1995; .Preparar tareas que evidencien habilidades con missingness (DR) Bang & Robins (2005), DRPT Spirtes et al. (2001) y contrafactual distributions Chernozhukov et al. (2009).Definir protocolo para manejar no respuesta/atrición en la evaluación (relevante por las fallas prácticas destacadas por West) Ioannidis (2001), Bang & Robins (2005).Paso 5. Preparar datasets y notebooks (1 semana).Incluir al menos un dataset con missingness (para DR) Bang & Robins (2005) y uno con outcome continuo apto para análisis distribucional Chernozhukov et al. (2009). Si se incluye time series, seleccionar series simples y documentar retos de ruido/alta dimensión Asesh (2022).Fase B — Implementación (6–10 semanas)Paso 6. Aplicar pre-test y recolectar línea base (semana 1 del módulo).Registrar también variables de contexto (experiencia previa) para interpretar heterogeneidad, coherente con la idea de que evidencia y magnitudes pueden variar por diseño/población Marcus et al. (2012), Ioannidis (2001).Paso 7. Dictar Módulos 1–3 (semanas 1–3).Entregar habilidades nucleares: especificación + DAG + diagnóstico de identificación (y do-cálculo como parte del marco de Pearl, si el nivel lo permite) (Pearl, 2009; , Pearl, 1995; .Paso 8. Dictar Módulos 4–5 (semanas 4–5).Conectar especificación con diseño realista (alternativas al RCT) Ioannidis (2001) y con interpretación crítica de discrepancias RCT vs no-RCT Marcus et al. (2012).Paso 9. Dictar Laboratorios 6A–6C (semanas 6–8).Missingness y doble robustez Bang & Robins (2005).Preferencia y DRPT Spirtes et al. (2001).Contrafactual distributions Chernozhukov et al. (2009).A lo largo, reforzar la traducción pregunta → estimando → supuestos (DAG) → método (Pearl, 2009; , Pearl, 1995; .Paso 10 (opcional). Módulo 7 de series temporales (semana 9).Enfatizar que “especificar” (dirección, lags, cadenas) es parte del problema bajo ruido/alta dimensión Asesh (2022), y conectar con lenguaje gráfico/estructura (Pearl, 2009; , West, 2009).Fase C — Cierre y escritura (3–5 semanas)Paso 11. Post-test + evaluación final de artefactos (1 semana).Aplicar rúbrica (ver sección 6) a fichas, DAGs, y notebooks; documentar faltantes/atrición como amenaza y cómo se manejó (en línea con Ioannidis (2001), Bang & Robins (2005)).Paso 12. Redacción del artículo y paquete reproducible (2–4 semanas).Estructura sugerida:Introducción (necesidad de lenguaje integrador: DAG como lenguaje (Pearl, 2009; ; unificación conceptual Pearl, 1995; ; puente a IA/búsqueda causal West, 2009)).Diseño curricular (módulos y racional: diseños alternativos Ioannidis (2001), discrepancias Marcus et al. (2012), missingness Bang & Robins (2005), preferencia Spirtes et al. (2001), distribuciones contrafactuales Chernozhukov et al. (2009), time series Asesh (2022)).Evaluación (pre/post y rúbrica) con discusión de amenazas (atrición/missingness) Ioannidis (2001), Bang & Robins (2005).Resultados (mejoras por LO y errores típicos).Discusión (implicaciones para docencia en estadística y ciencia de datos; límites y próximos pasos).6. Rúbrica de evaluación: criterio yace demasiado largo...